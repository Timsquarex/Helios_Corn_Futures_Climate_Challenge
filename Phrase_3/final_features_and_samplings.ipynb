{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6a45d",
   "metadata": {},
   "source": [
    "#### Data Loading and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451190d",
   "metadata": {},
   "source": [
    "- Note: Change your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskfutures = pd.read_csv('/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/corn_climate_risk_futures_daily_master.csv')\n",
    "marketshare = pd.read_csv('/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/corn_regional_market_share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = riskfutures.copy()\n",
    "mergedf['day_of_year'] = pd.to_datetime(mergedf['date_on'],format='%Y-%m-%d').dt.dayofyear\n",
    "mergedf['quarter'] = pd.to_datetime(mergedf['date_on'],format='%Y-%m-%d').dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = mergedf.merge(marketshare[['region_id','percent_country_production']],how='left',on='region_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf['percent_country_production'] = mergedf['percent_country_production'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547c1ee",
   "metadata": {},
   "source": [
    "#### Introduction of Climate Risk (Coldwave) by Tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cnt locations for each rows\n",
    "mergedf['total_location_by_region'] = mergedf['climate_risk_cnt_locations_heat_stress_risk_low'] + \\\n",
    "                                    mergedf['climate_risk_cnt_locations_heat_stress_risk_medium'] + \\\n",
    "                                    mergedf['climate_risk_cnt_locations_heat_stress_risk_high']\n",
    "\n",
    "# Climate Risk for Coldwave, and Flood:\n",
    "for i in range(1, 5):\n",
    "    mergedf[f'medium_coldstress_lag_{i}'] = mergedf['climate_risk_cnt_locations_unseasonably_cold_risk_medium'].shift(i)\n",
    "    mergedf[f'medium_coldstress_lag_{i}'] = mergedf[f'medium_coldstress_lag_{i}'].fillna(0)\n",
    "\n",
    "for j in range(1, 3): \n",
    "    mergedf[f'high_coldstress_lag_{j}'] = mergedf['climate_risk_cnt_locations_unseasonably_cold_risk_high'].shift(j)\n",
    "    mergedf[f'high_coldstress_lag_{j}'] = mergedf[f'high_coldstress_lag_{j}'].fillna(0)\n",
    "\n",
    "mergedf['medium_coldstress_4days_average'] = mergedf[[f'medium_coldstress_lag_{i}' for i in range(1, 5)]].mean(axis=1)\n",
    "mergedf['medium_coldstress_2days_average'] = mergedf[[f'medium_coldstress_lag_{i}' for i in range(1, 3)]].mean(axis=1)\n",
    "mergedf['high_coldstress_2days_average'] = mergedf[[f'high_coldstress_lag_{i}' for i in range(1, 3)]].mean(axis=1)\n",
    "\n",
    "mergedf['climate_risk_cnt_locations_coldwave_risk_high'] = (mergedf['medium_coldstress_4days_average'] + mergedf['high_coldstress_2days_average']) / 2\n",
    "mergedf['climate_risk_cnt_locations_coldwave_risk_medium'] = (mergedf['medium_coldstress_2days_average'] + mergedf['high_coldstress_lag_1']) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07758",
   "metadata": {},
   "source": [
    "### 1 Baseline Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d9907",
   "metadata": {},
   "source": [
    "#### 1.1 Production-Weighted Risk Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3421b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_categories = ['heat_stress', 'unseasonably_cold', 'excess_precip', 'drought', 'coldwave']\n",
    "for risk in risk_categories:\n",
    "    medium = f'climate_risk_cnt_locations_{risk}_risk_medium'\n",
    "    high = f'climate_risk_cnt_locations_{risk}_risk_high'\n",
    "    \n",
    "    risk_scores = (1*mergedf[medium]+2*mergedf[high])/\\\n",
    "                           (mergedf['total_location_by_region'])\n",
    "    ## define regional daily risk score as normalized weighted sum of number of locations\n",
    "    \n",
    "    production_weighted_risk_scores = (risk_scores*mergedf['percent_country_production'])/100\n",
    "    ## use marketshare data to get production-weighted regional daily risk scores\n",
    "    \n",
    "    mergedf[f'climate_risk_{risk}_score'] = risk_scores\n",
    "    mergedf[f'climate_risk_{risk}_weighted_score'] = production_weighted_risk_scores\n",
    "    ## iterate for all five climate risk types; total 10 new engieered features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03339428",
   "metadata": {},
   "source": [
    "#### 1.2 Composite Risk Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf['climate_risk_temperature_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories[:2]]].max(axis=1) \n",
    "## maximum of temperature-related risk scores\n",
    "mergedf['climate_risk_precipitation_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories[2:4]]].max(axis=1)\n",
    "## maximum of precipitation-related risk scores\n",
    "mergedf['climate_risk_overall_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories]].max(axis=1)\n",
    "## maximum of all risk scores\n",
    "mergedf['climate_risk_avg_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories]].mean(axis=1)\n",
    "## average of all risk scores\n",
    "## total 4 new engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62c087",
   "metadata": {},
   "source": [
    "#### 1.3 Risk Temporal Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf413a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = mergedf.sort_values(['region_name','date_on'])\n",
    "window_period = [7,14,30,60,90,120,240]\n",
    "## three periods to compute risk scores moving avg and maximum \n",
    "for window in window_period:\n",
    "    for risk in risk_categories:\n",
    "        mergedf[f'climate_risk_{risk}_ma_{window}d'] = \\\n",
    "        mergedf.groupby(['region_name'])[f'climate_risk_{risk}_score']\\\n",
    "               .rolling(window=window,min_periods=1).mean().reset_index(level=0,drop=True)\n",
    "## compute risk score moving avg with different windows for different risk types in each region\n",
    "\n",
    "        mergedf[f'climate_risk_{risk}_max_{window}d'] = \\\n",
    "        mergedf.groupby(['region_name'])[f'climate_risk_{risk}_score']\\\n",
    "               .rolling(window=window,min_periods=1).max().reset_index(level=0,drop=True)\n",
    "## compute maximum risk scores with different windows for different risk types in each region\n",
    "## total 7*5*2 = 70 new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047fc1c",
   "metadata": {},
   "source": [
    "#### 1.4 Risk Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_change1d = mergedf.groupby('region_name')[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    "       .diff(periods=1)\\\n",
    "       .rename(columns=dict(zip([f'climate_risk_{risk}_score' for risk in risk_categories],\\\n",
    "                                [f'climate_risk_{risk}_change_1d' for risk in risk_categories])))\n",
    "## Daily Change of risk scores for each risk type in each region \n",
    "\n",
    "features_acceleration = features_change1d.diff(periods=1)\n",
    "## Acceleration of daily Change of risk scores for each risk type in each region\n",
    "\n",
    "features_change1w = mergedf.groupby('region_name')[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    "       .diff(periods=7)\\\n",
    "       .rename(columns=dict(zip([f'climate_risk_{risk}_score' for risk in risk_categories],\\\n",
    "                                [f'climate_risk_{risk}_change_1d' for risk in risk_categories])))\n",
    "## Weekly Change of risk scores for each risk type in each region \n",
    "\n",
    "mergedf = pd.concat([mergedf,\\\n",
    "           features_change1d,\\\n",
    "           features_change1w,\\\n",
    "           features_acceleration],axis=1)\n",
    "## 15 new features in Risk Momentum category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e875c46",
   "metadata": {},
   "source": [
    "#### 1.5 Cross-Regional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_country = pd.concat([\\\n",
    "mergedf.groupby(['country_name', 'date_on'])\\\n",
    "[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    ".agg(['mean','max','std']),\n",
    "## compute country-wide daily avg, max, and std risk scores\n",
    "mergedf.groupby(['country_name', 'date_on'])\\\n",
    "[[f'climate_risk_{risk}_weighted_score' for risk in risk_categories]]\\\n",
    ".agg('sum')],axis=1)\n",
    "## compute country-wide daily production-weighted sum risk scores\n",
    "feature_country.columns = [f'climate_risk_{risk}_score_country_{metric}'\\\n",
    "                          for risk in risk_categories \\\n",
    "                          for metric in ['mean','max','std']]+\\\n",
    "                          [f'climate_risk_{risk}_weighted_score_country_sum'\\\n",
    "                          for risk in risk_categories]\n",
    "## rename new features\n",
    "mergedf = mergedf.merge(feature_country.reset_index(),\\\n",
    "              how='left',\\\n",
    "              on=['country_name','date_on'])\n",
    "## add 4*5=20 new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f068896",
   "metadata": {},
   "source": [
    "#### Columns to Drop (Just select the drought, excess precipitation, and coldwave risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6809d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['climate_risk_cnt_locations_heat_stress_risk_low',\n",
    " 'climate_risk_cnt_locations_heat_stress_risk_medium',\n",
    " 'climate_risk_cnt_locations_heat_stress_risk_high',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_low',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_medium',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_high',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_low',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_medium',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_high',\n",
    " 'climate_risk_cnt_locations_drought_risk_low',\n",
    " 'climate_risk_cnt_locations_drought_risk_medium',\n",
    " 'climate_risk_cnt_locations_drought_risk_high',\n",
    " 'climate_risk_cnt_locations_coldwave_risk_high',\n",
    " 'climate_risk_cnt_locations_coldwave_risk_medium']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1826745",
   "metadata": {},
   "source": [
    "#### Non-Linear Transformation by William and Tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = mergedf.columns[mergedf.columns.duplicated()].tolist()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6503697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "new_cols = []\n",
    "\n",
    "for col in mergedf.columns:\n",
    "    counts[col] += 1\n",
    "    if counts[col] == 1:\n",
    "        new_cols.append(col)       # keep first occurrence as-is\n",
    "    else:\n",
    "        new_cols.append(f\"{col}__{counts[col]}\")  # suffix duplicates\n",
    "\n",
    "mergedf.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf_1 = mergedf.copy()\n",
    "mergedf_1 = mergedf.dropna()\n",
    "print(mergedf_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pl.from_pandas(mergedf)\n",
    "del mergedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58413207",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7888bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = []\n",
    "\n",
    "for feature_name in climate_risk_selected_cols:\n",
    "    col = pl.col(feature_name)\n",
    "    \n",
    "    # log1p of non-negative values, fill nulls\n",
    "    exprs.append(\n",
    "        pl.when(col >= 0)\n",
    "          .then((col + 1).log()) \n",
    "          .otherwise(0)\n",
    "          .alias(f\"{feature_name}_log1p\")\n",
    "    )\n",
    "\n",
    "    # signed sqrt\n",
    "    exprs.append(\n",
    "        (col.sign() * col.abs().sqrt()).fill_null(0).alias(f\"{feature_name}_ssqrt\")\n",
    "    )\n",
    "\n",
    "    # threshold magnitude (>1)\n",
    "    exprs.append(\n",
    "        pl.when(col > 1)\n",
    "          .then(col)\n",
    "          .otherwise(0)\n",
    "          .alias(f\"{feature_name}_thresh_mag\")\n",
    "    )\n",
    "\n",
    "    # tangent\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).tan().fill_null(0).alias(f\"{feature_name}_tangent\")\n",
    "    )\n",
    "\n",
    "    # sine\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).sin().fill_null(0).alias(f\"{feature_name}_sin\")\n",
    "    )\n",
    "\n",
    "    # cosine\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).cos().fill_null(0).alias(f\"{feature_name}_cos\")\n",
    "    )\n",
    "\n",
    "# Add all new columns at once\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_multiplier = [1, 2, 3]\n",
    "exprs = []\n",
    "\n",
    "for multiplier in std_multiplier:\n",
    "    for col_name in climate_risk_selected_cols:\n",
    "        col = pl.col(col_name)\n",
    "        # compute threshold: keep values > multiplier * std, else 0\n",
    "        exprs.append(\n",
    "            pl.when(col > multiplier * col.std())\n",
    "              .then(col)\n",
    "              .otherwise(0)\n",
    "              .alias(f\"{col_name}_above_{multiplier}_std\")\n",
    "        )\n",
    "\n",
    "# add all new columns at once\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pldf.sort([\"region_name\", \"date_on\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbf4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_period = [7, 14, 30]\n",
    "\n",
    "exprs = []\n",
    "\n",
    "for window in window_period:\n",
    "    for col_name in climate_risk_selected_cols:\n",
    "        exprs.append(\n",
    "            pl.col(col_name)\n",
    "              .shift(window)\n",
    "              .alias(f\"{col_name}_lag_{window}d\")\n",
    "        )\n",
    "\n",
    "# Add lag features to mergedf\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pldf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef394f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [c for c in pldf.columns if c.startswith('futures_')]\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc1718",
   "metadata": {},
   "source": [
    "### Samplings into 20 parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7861b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/kaggle/working/samples/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for i in range(20):\n",
    "    sample = (\n",
    "        pldf\n",
    "          .sample(n=10_000, seed=i)\n",
    "    )\n",
    "    \n",
    "    pdf = sample.to_pandas()\n",
    "    pdf.to_parquet(os.path.join(out_dir, f\"sample_{i}.parquet\"))\n",
    "\n",
    "    del sample, pdf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
