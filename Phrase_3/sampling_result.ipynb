{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31630e48",
   "metadata": {},
   "source": [
    "### Doing experiment for a random sample first, then extend to a for loop (Please change your file path and do the samplings in another jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = pd.read_parquet('/kaggle/input/helios-random-sampling-datasets/helios_random_samplings/sample_0.parquet')\n",
    "testing_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbd369",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_cols = [c for c in testing_dataset.columns if c.startswith('futures_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dbcb4",
   "metadata": {},
   "source": [
    "### First filter: We only select the features have above 0.6 avg_sig_corr for the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_features_df = pd.read_csv('/kaggle/input/final-corr/final_result_corr_2.csv') #\n",
    "potential_features = potential_features_df.loc[\n",
    "    potential_features_df['avg_sig_corr'] >= 0.6,\n",
    "    'climate_variable'\n",
    "].tolist()\n",
    "print(len(potential_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_cols = ['crop_name','country_name','date_on_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cols = necessary_cols + futures_cols + potential_features\n",
    "print(len(full_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf248b",
   "metadata": {},
   "source": [
    "### Load the CFCS backtest functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partial_correlations(df,by=['crop_name','country_name','date_on_month']):\n",
    "    ## df: must be a dataframe with non-zero rows and at least two columns \n",
    "    ## each with name starting with climate_risk and futures\n",
    "    \n",
    "    ## by: groupby keyword value to partition df to compute partial\n",
    "    ## correlations of each group\n",
    "    ## default to groupings shown in Helios sample notebook\n",
    "\n",
    "    def _climate_futures_corr_table(df,climate_risk_columns,futures_columns):\n",
    "        ## compute correlations of each climate-futures variable pair\n",
    "        ## Note: dataframe must contain at least two non-null pairs\n",
    "        ## to produce a non-null correlation\n",
    "        \n",
    "        corr_matrix = df[climate_risk_columns+futures_columns]\\\n",
    "                      .corr(method='pearson',min_periods=2,numeric_only=True)\n",
    "        ## corr() auto drop nan values before computing\n",
    "        ## number pairs must be at least two for computation\n",
    "        ## according to formula\n",
    "        corr_table = corr_matrix.loc[climate_risk_columns,futures_columns]\\\n",
    "        .rename_axis(index='climate_variable',columns='futures_variable')\\\n",
    "        .stack().reset_index(name='correlation')\n",
    "        ## drop nan correlations in stack operation\n",
    "        return corr_table.round(5)\n",
    "\n",
    "    climate_risk_columns = [c for c in df.columns if c.startswith('climate_risk')]\n",
    "    futures_columns = [c for c in df.columns if c.startswith('futures')]\n",
    "    df_default = pd.DataFrame([],columns=['correlation'])\n",
    "    \n",
    "    if len(climate_risk_columns)==0:\n",
    "        print('input dataframe must have at least one column with name starting with climate_risk')\n",
    "        return df_default\n",
    "    \n",
    "    if len(futures_columns)==0:\n",
    "        print('input dataframe must have at least one column with name starting with futures')\n",
    "        return df_default\n",
    "    \n",
    "    if by==None:\n",
    "        corr_tables = _climate_futures_corr_table(df,climate_risk_columns,futures_columns)\n",
    "    else:\n",
    "        try:\n",
    "            corr_tables = df.groupby(by=by).apply(_climate_futures_corr_table,\\\n",
    "                                                  climate_risk_columns,\\\n",
    "                                                  futures_columns,\\\n",
    "                                                  include_groups=False\n",
    "                                                 )\n",
    "            corr_tables = corr_tables\\\n",
    "                          .reset_index(level=len(corr_tables.index.levels)-1,drop=True)\\\n",
    "                          .reset_index()\n",
    "            ## compute and combine the correlation table for each group\n",
    "        except KeyError:\n",
    "            print('illegal by values')\n",
    "            return df_default\n",
    "\n",
    "    return corr_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270168ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfcs(df):\n",
    "    \"\"\"\n",
    "    Calculate the Climate-Futures Correlation Score (CFCS) for leaderboard ranking.\n",
    "    \n",
    "    CFCS = (0.5 × Avg_Sig_Corr_Score) + (0.3 × Max_Corr_Score) + (0.2 × Sig_Count_Score)\n",
    "\n",
    "    Input dataframe must have correlation column for computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove null correlations\n",
    "    valid_corrs = df[\"correlation\"].dropna()\n",
    "    \n",
    "    if len(valid_corrs) == 0:\n",
    "        return {'cfcs_score': 0.0, 'error': 'No valid correlations'}\n",
    "    \n",
    "    # Calculate base metrics\n",
    "    abs_corrs = valid_corrs.abs()\n",
    "    max_abs_corr = abs_corrs.max()\n",
    "    significant_corrs = abs_corrs[abs_corrs >= 0.5]\n",
    "    significant_count = len(significant_corrs)\n",
    "    total_count = len(valid_corrs)\n",
    "    \n",
    "    # Calculate component scores - ONLY average significant correlations\n",
    "    if significant_count > 0:\n",
    "        avg_sig_corr = significant_corrs.mean()\n",
    "        avg_sig_score = min(100, avg_sig_corr * 100)  # Cap at 100 when avg sig reaches 1.0\n",
    "    else:\n",
    "        avg_sig_corr = 0.0\n",
    "        avg_sig_score = 0.0\n",
    "    \n",
    "    max_corr_score = min(100, max_abs_corr * 100)  # Cap at 100 when max reaches 1.0\n",
    "    sig_count_score = (significant_count / total_count) * 100  # Percentage\n",
    "    \n",
    "    # Composite score: Focus more on quality of significant correlations\n",
    "    cfcs = (0.5 * avg_sig_score) + (0.3 * max_corr_score) + (0.2 * sig_count_score)\n",
    "    scoreboard = {'cfcs_score': cfcs, 'avg_sig_score': avg_sig_score,\\\n",
    "                  'max_corr_score': max_corr_score,\\\n",
    "                  'sig_count_score': sig_count_score\n",
    "                 }\n",
    "    print(f'{round(sig_count_score,2)}% of all correlations are significant')\n",
    "    print(f'Average significant correlation is {round(avg_sig_corr,3)}')\n",
    "    print(f'highest absolute correlation found is {round(max_abs_corr,3)}')\n",
    "    print(f'final CFCS score is {round(cfcs,2)}')\n",
    "    return scoreboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70321e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigcorr_report(df, features='climate_variable', sig_level=0.5):\n",
    "    '''\n",
    "        Generate a CFCS sub scores report for each feature\n",
    "        with significant correlations.\n",
    "        \n",
    "        Input dataframe must contain a correlation column and feature columns.\n",
    "    \n",
    "        Returns a DataFrame with columns:\n",
    "        ['climate_variable', 'avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']\n",
    "    '''\n",
    "    \n",
    "    # Absolute correlation\n",
    "    df['correlation_abs'] = df.correlation.abs()\n",
    "    \n",
    "    try:\n",
    "        # Mask correlations below threshold\n",
    "        df.loc[df['correlation_abs'] < sig_level, ['correlation_abs']] = np.nan\n",
    "        \n",
    "        # Group by feature\n",
    "        reportdf = df.groupby(features).agg({\n",
    "            'correlation_abs': ['mean','max','count'],\n",
    "            'correlation': 'count'\n",
    "        })\n",
    "        \n",
    "    except TypeError:\n",
    "        print('sig_level must be a number between 0 and 1')\n",
    "        return None\n",
    "    except KeyError:\n",
    "        print('illegal features values')\n",
    "        return None\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    reportdf.columns = ['avg_sig_corr','max_sig_corr','sig_corr_count','total_corr_count']\n",
    "    \n",
    "    # Compute ratio\n",
    "    reportdf['sig_corr_ratio(%)'] = 100 * reportdf['sig_corr_count'] / reportdf['total_corr_count']\n",
    "    \n",
    "    # Keep only rows with avg_sig_corr not null\n",
    "    reportdf = reportdf[reportdf['avg_sig_corr'].notnull()]\\\n",
    "                       .loc[:, ['avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']]\\\n",
    "                       .round(3)\n",
    "    \n",
    "    # Add the feature name as a column\n",
    "    reportdf[features] = reportdf.index\n",
    "    reportdf = reportdf.reset_index(drop=True)  # optional: reset index\n",
    "    \n",
    "    # Reorder columns so feature name is first\n",
    "    reportdf = reportdf[[features, 'avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']]\n",
    "    \n",
    "    return reportdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a492ecf",
   "metadata": {},
   "source": [
    "### For loop, and get the features are performing consistently in these 20 parquet files, by sorting avg_sig_corr (18 features) with listing out the first 220 and sig_count_count (2 features) by listing out the first 100 \n",
    "\n",
    "#### At the end of the subset, because of I missed some features, ended up we just have 15 features in our subset (final_submission.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7899db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_dir = '/kaggle/working/sampling_result/'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97511674",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_set = None\n",
    "\n",
    "for i in range(20):\n",
    "    testings = pd.read_csv(f'/kaggle/input/helios-samplings-result/result_{i}')\n",
    "    top_30_columns = (\n",
    "        testings\n",
    "        .sort_values(by='avg_sig_corr', ascending=False)\n",
    "        .head(220)['climate_variable']\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    current_set = set(top_30_columns)\n",
    "\n",
    "    if storage_set is None:\n",
    "        storage_set = current_set\n",
    "    else:\n",
    "        storage_set = storage_set.intersection(current_set)\n",
    "\n",
    "storage_list = list(storage_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(storage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6aed3",
   "metadata": {},
   "source": [
    "### This is the avg_sig_corr features selected for final_submission:\n",
    "'climate_risk_excess_precip_max_240d_cos_lag_7d',\n",
    "'climate_risk_excess_precip_max_240d_ssqrt_above_3_std_lag_7d','climate_risk_excess_precip_max_240d_above_3_std_lag_14d',\n",
    "'climate_risk_excess_precip_max_240d_above_3_std_lag_30d',\n",
    "'climate_risk_drought_ma_90d_lag_7d',\n",
    "'climate_risk_drought_ma_120d_tangent',\n",
    "'climate_risk_drought_ma_120d_cos',\n",
    "'climate_risk_drought_ma_120d_cos_lag_7d',\n",
    "'climate_risk_drought_ma_90d_cos_lag_30d',\n",
    "'climate_risk_drought_ma_120d_cos_above_1_std',\n",
    "'climate_risk_drought_ma_120d_cos_above_2_std',\n",
    "'climate_risk_drought_ma_60d_cos_above_2_std_lag_30d',\n",
    "'climate_risk_drought_ma_120d_cos_above_3_std_lag_7d','climate_risk_coldwave_max_60d_thresh_mag_lag_30d',\n",
    "'climate_risk_coldwave_max_60d_thresh_mag_above_1_std_lag_30d'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9762",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
