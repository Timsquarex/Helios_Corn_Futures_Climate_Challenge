{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6a45d",
   "metadata": {},
   "source": [
    "#### Data Loading and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451190d",
   "metadata": {},
   "source": [
    "- Note: Change your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskfutures = pd.read_csv('/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/corn_climate_risk_futures_daily_master.csv')\n",
    "marketshare = pd.read_csv('/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/corn_regional_market_share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = riskfutures.copy()\n",
    "mergedf['day_of_year'] = pd.to_datetime(mergedf['date_on'],format='%Y-%m-%d').dt.dayofyear\n",
    "mergedf['quarter'] = pd.to_datetime(mergedf['date_on'],format='%Y-%m-%d').dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = mergedf.merge(marketshare[['region_id','percent_country_production']],how='left',on='region_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf['percent_country_production'] = mergedf['percent_country_production'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547c1ee",
   "metadata": {},
   "source": [
    "#### Introduction of Climate Risk (Coldwave) by Tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total cnt locations for each rows\n",
    "mergedf['total_location_by_region'] = mergedf['climate_risk_cnt_locations_heat_stress_risk_low'] + \\\n",
    "                                    mergedf['climate_risk_cnt_locations_heat_stress_risk_medium'] + \\\n",
    "                                    mergedf['climate_risk_cnt_locations_heat_stress_risk_high']\n",
    "\n",
    "# Climate Risk for Coldwave, and Flood:\n",
    "for i in range(1, 5):\n",
    "    mergedf[f'medium_coldstress_lag_{i}'] = mergedf['climate_risk_cnt_locations_unseasonably_cold_risk_medium'].shift(i)\n",
    "    mergedf[f'medium_coldstress_lag_{i}'] = mergedf[f'medium_coldstress_lag_{i}'].fillna(0)\n",
    "\n",
    "for j in range(1, 3): \n",
    "    mergedf[f'high_coldstress_lag_{j}'] = mergedf['climate_risk_cnt_locations_unseasonably_cold_risk_high'].shift(j)\n",
    "    mergedf[f'high_coldstress_lag_{j}'] = mergedf[f'high_coldstress_lag_{j}'].fillna(0)\n",
    "\n",
    "mergedf['medium_coldstress_4days_average'] = mergedf[[f'medium_coldstress_lag_{i}' for i in range(1, 5)]].mean(axis=1)\n",
    "mergedf['medium_coldstress_2days_average'] = mergedf[[f'medium_coldstress_lag_{i}' for i in range(1, 3)]].mean(axis=1)\n",
    "mergedf['high_coldstress_2days_average'] = mergedf[[f'high_coldstress_lag_{i}' for i in range(1, 3)]].mean(axis=1)\n",
    "\n",
    "mergedf['climate_risk_cnt_locations_coldwave_risk_high'] = (mergedf['medium_coldstress_4days_average'] + mergedf['high_coldstress_2days_average']) / 2\n",
    "mergedf['climate_risk_cnt_locations_coldwave_risk_medium'] = (mergedf['medium_coldstress_2days_average'] + mergedf['high_coldstress_lag_1']) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07758",
   "metadata": {},
   "source": [
    "### 1 Baseline Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d9907",
   "metadata": {},
   "source": [
    "#### 1.1 Production-Weighted Risk Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3421b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_categories = ['heat_stress', 'unseasonably_cold', 'excess_precip', 'drought', 'coldwave']\n",
    "for risk in risk_categories:\n",
    "    medium = f'climate_risk_cnt_locations_{risk}_risk_medium'\n",
    "    high = f'climate_risk_cnt_locations_{risk}_risk_high'\n",
    "    \n",
    "    risk_scores = (1*mergedf[medium]+2*mergedf[high])/\\\n",
    "                           (mergedf['total_location_by_region'])\n",
    "    ## define regional daily risk score as normalized weighted sum of number of locations\n",
    "    \n",
    "    production_weighted_risk_scores = (risk_scores*mergedf['percent_country_production'])/100\n",
    "    ## use marketshare data to get production-weighted regional daily risk scores\n",
    "    \n",
    "    mergedf[f'climate_risk_{risk}_score'] = risk_scores\n",
    "    mergedf[f'climate_risk_{risk}_weighted_score'] = production_weighted_risk_scores\n",
    "    ## iterate for all five climate risk types; total 10 new engieered features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03339428",
   "metadata": {},
   "source": [
    "#### 1.2 Composite Risk Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf['climate_risk_temperature_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories[:2]]].max(axis=1) \n",
    "## maximum of temperature-related risk scores\n",
    "mergedf['climate_risk_precipitation_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories[2:4]]].max(axis=1)\n",
    "## maximum of precipitation-related risk scores\n",
    "mergedf['climate_risk_overall_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories]].max(axis=1)\n",
    "## maximum of all risk scores\n",
    "mergedf['climate_risk_avg_stress'] = \\\n",
    "mergedf[[f'climate_risk_{risk}_score' for risk in risk_categories]].mean(axis=1)\n",
    "## average of all risk scores\n",
    "## total 4 new engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62c087",
   "metadata": {},
   "source": [
    "#### 1.3 Risk Temporal Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf413a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf = mergedf.sort_values(['region_name','date_on'])\n",
    "window_period = [7,14,30,60,90,120,240]\n",
    "## three periods to compute risk scores moving avg and maximum \n",
    "for window in window_period:\n",
    "    for risk in risk_categories:\n",
    "        mergedf[f'climate_risk_{risk}_ma_{window}d'] = \\\n",
    "        mergedf.groupby(['region_name'])[f'climate_risk_{risk}_score']\\\n",
    "               .rolling(window=window,min_periods=1).mean().reset_index(level=0,drop=True)\n",
    "## compute risk score moving avg with different windows for different risk types in each region\n",
    "\n",
    "        mergedf[f'climate_risk_{risk}_max_{window}d'] = \\\n",
    "        mergedf.groupby(['region_name'])[f'climate_risk_{risk}_score']\\\n",
    "               .rolling(window=window,min_periods=1).max().reset_index(level=0,drop=True)\n",
    "## compute maximum risk scores with different windows for different risk types in each region\n",
    "## total 7*5*2 = 70 new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047fc1c",
   "metadata": {},
   "source": [
    "#### 1.4 Risk Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_change1d = mergedf.groupby('region_name')[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    "       .diff(periods=1)\\\n",
    "       .rename(columns=dict(zip([f'climate_risk_{risk}_score' for risk in risk_categories],\\\n",
    "                                [f'climate_risk_{risk}_change_1d' for risk in risk_categories])))\n",
    "## Daily Change of risk scores for each risk type in each region \n",
    "\n",
    "features_acceleration = features_change1d.diff(periods=1)\n",
    "## Acceleration of daily Change of risk scores for each risk type in each region\n",
    "\n",
    "features_change1w = mergedf.groupby('region_name')[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    "       .diff(periods=7)\\\n",
    "       .rename(columns=dict(zip([f'climate_risk_{risk}_score' for risk in risk_categories],\\\n",
    "                                [f'climate_risk_{risk}_change_1d' for risk in risk_categories])))\n",
    "## Weekly Change of risk scores for each risk type in each region \n",
    "\n",
    "mergedf = pd.concat([mergedf,\\\n",
    "           features_change1d,\\\n",
    "           features_change1w,\\\n",
    "           features_acceleration],axis=1)\n",
    "## 15 new features in Risk Momentum category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e875c46",
   "metadata": {},
   "source": [
    "#### 1.5 Cross-Regional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_country = pd.concat([\\\n",
    "mergedf.groupby(['country_name', 'date_on'])\\\n",
    "[[f'climate_risk_{risk}_score' for risk in risk_categories]]\\\n",
    ".agg(['mean','max','std']),\n",
    "## compute country-wide daily avg, max, and std risk scores\n",
    "mergedf.groupby(['country_name', 'date_on'])\\\n",
    "[[f'climate_risk_{risk}_weighted_score' for risk in risk_categories]]\\\n",
    ".agg('sum')],axis=1)\n",
    "## compute country-wide daily production-weighted sum risk scores\n",
    "feature_country.columns = [f'climate_risk_{risk}_score_country_{metric}'\\\n",
    "                          for risk in risk_categories \\\n",
    "                          for metric in ['mean','max','std']]+\\\n",
    "                          [f'climate_risk_{risk}_weighted_score_country_sum'\\\n",
    "                          for risk in risk_categories]\n",
    "## rename new features\n",
    "mergedf = mergedf.merge(feature_country.reset_index(),\\\n",
    "              how='left',\\\n",
    "              on=['country_name','date_on'])\n",
    "## add 4*5=20 new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f068896",
   "metadata": {},
   "source": [
    "#### Columns to Drop (Just select the drought, excess precipitation, and coldwave risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6809d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['climate_risk_cnt_locations_heat_stress_risk_low',\n",
    " 'climate_risk_cnt_locations_heat_stress_risk_medium',\n",
    " 'climate_risk_cnt_locations_heat_stress_risk_high',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_low',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_medium',\n",
    " 'climate_risk_cnt_locations_unseasonably_cold_risk_high',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_low',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_medium',\n",
    " 'climate_risk_cnt_locations_excess_precip_risk_high',\n",
    " 'climate_risk_cnt_locations_drought_risk_low',\n",
    " 'climate_risk_cnt_locations_drought_risk_medium',\n",
    " 'climate_risk_cnt_locations_drought_risk_high',\n",
    " 'climate_risk_cnt_locations_coldwave_risk_high',\n",
    " 'climate_risk_cnt_locations_coldwave_risk_medium']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1826745",
   "metadata": {},
   "source": [
    "#### Non-Linear Transformation by William and Tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = mergedf.columns[mergedf.columns.duplicated()].tolist()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6503697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counts = collections.defaultdict(int)\n",
    "new_cols = []\n",
    "\n",
    "for col in mergedf.columns:\n",
    "    counts[col] += 1\n",
    "    if counts[col] == 1:\n",
    "        new_cols.append(col)       # keep first occurrence as-is\n",
    "    else:\n",
    "        new_cols.append(f\"{col}__{counts[col]}\")  # suffix duplicates\n",
    "\n",
    "mergedf.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedf_1 = mergedf.copy()\n",
    "mergedf_1 = mergedf.dropna()\n",
    "print(mergedf_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pl.from_pandas(mergedf)\n",
    "del mergedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58413207",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7888bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = []\n",
    "\n",
    "for feature_name in climate_risk_selected_cols:\n",
    "    col = pl.col(feature_name)\n",
    "    \n",
    "    # log1p of non-negative values, fill nulls\n",
    "    exprs.append(\n",
    "        pl.when(col >= 0)\n",
    "          .then((col + 1).log()) \n",
    "          .otherwise(0)\n",
    "          .alias(f\"{feature_name}_log1p\")\n",
    "    )\n",
    "\n",
    "    # signed sqrt\n",
    "    exprs.append(\n",
    "        (col.sign() * col.abs().sqrt()).fill_null(0).alias(f\"{feature_name}_ssqrt\")\n",
    "    )\n",
    "\n",
    "    # threshold magnitude (>1)\n",
    "    exprs.append(\n",
    "        pl.when(col > 1)\n",
    "          .then(col)\n",
    "          .otherwise(0)\n",
    "          .alias(f\"{feature_name}_thresh_mag\")\n",
    "    )\n",
    "\n",
    "    # tangent\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).tan().fill_null(0).alias(f\"{feature_name}_tangent\")\n",
    "    )\n",
    "\n",
    "    # sine\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).sin().fill_null(0).alias(f\"{feature_name}_sin\")\n",
    "    )\n",
    "\n",
    "    # cosine\n",
    "    exprs.append(\n",
    "        pl.col(feature_name).cos().fill_null(0).alias(f\"{feature_name}_cos\")\n",
    "    )\n",
    "\n",
    "# Add all new columns at once\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_multiplier = [1, 2, 3]\n",
    "exprs = []\n",
    "\n",
    "for multiplier in std_multiplier:\n",
    "    for col_name in climate_risk_selected_cols:\n",
    "        col = pl.col(col_name)\n",
    "        # compute threshold: keep values > multiplier * std, else 0\n",
    "        exprs.append(\n",
    "            pl.when(col > multiplier * col.std())\n",
    "              .then(col)\n",
    "              .otherwise(0)\n",
    "              .alias(f\"{col_name}_above_{multiplier}_std\")\n",
    "        )\n",
    "\n",
    "# add all new columns at once\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith('climate_risk_')]\n",
    "climate_risk_selected_cols = [item for item in climate_risk_cols if item not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pldf.sort([\"region_name\", \"date_on\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbf4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_period = [7, 14, 30]\n",
    "\n",
    "exprs = []\n",
    "\n",
    "for window in window_period:\n",
    "    for col_name in climate_risk_selected_cols:\n",
    "        exprs.append(\n",
    "            pl.col(col_name)\n",
    "              .shift(window)\n",
    "              .alias(f\"{col_name}_lag_{window}d\")\n",
    "        )\n",
    "\n",
    "# Add lag features to mergedf\n",
    "pldf = pldf.with_columns(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pldf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef394f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [c for c in pldf.columns if c.startswith('futures_')]\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc1718",
   "metadata": {},
   "source": [
    "### Run the overall cfcs score for all features ~13000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7861b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partial_correlations(df,by=['crop_name','country_name','date_on_month']):\n",
    "    ## df: must be a dataframe with non-zero rows and at least two columns \n",
    "    ## each with name starting with climate_risk and futures\n",
    "    \n",
    "    ## by: groupby keyword value to partition df to compute partial\n",
    "    ## correlations of each group\n",
    "    ## default to groupings shown in Helios sample notebook\n",
    "\n",
    "    def _climate_futures_corr_table(df,climate_risk_columns,futures_columns):\n",
    "        ## compute correlations of each climate-futures variable pair\n",
    "        ## Note: dataframe must contain at least two non-null pairs\n",
    "        ## to produce a non-null correlation\n",
    "        \n",
    "        corr_matrix = df[climate_risk_columns+futures_columns]\\\n",
    "                      .corr(method='pearson',min_periods=2,numeric_only=True)\n",
    "        ## corr() auto drop nan values before computing\n",
    "        ## number pairs must be at least two for computation\n",
    "        ## according to formula\n",
    "        corr_table = corr_matrix.loc[climate_risk_columns,futures_columns]\\\n",
    "        .rename_axis(index='climate_variable',columns='futures_variable')\\\n",
    "        .stack().reset_index(name='correlation')\n",
    "        ## drop nan correlations in stack operation\n",
    "        return corr_table.round(5)\n",
    "\n",
    "    climate_risk_columns = [c for c in df.columns if c.startswith('climate_risk')]\n",
    "    futures_columns = [c for c in df.columns if c.startswith('futures')]\n",
    "    df_default = pd.DataFrame([],columns=['correlation'])\n",
    "    \n",
    "    if len(climate_risk_columns)==0:\n",
    "        print('input dataframe must have at least one column with name starting with climate_risk')\n",
    "        return df_default\n",
    "    \n",
    "    if len(futures_columns)==0:\n",
    "        print('input dataframe must have at least one column with name starting with futures')\n",
    "        return df_default\n",
    "    \n",
    "    if by==None:\n",
    "        corr_tables = _climate_futures_corr_table(df,climate_risk_columns,futures_columns)\n",
    "    else:\n",
    "        try:\n",
    "            corr_tables = df.groupby(by=by).apply(_climate_futures_corr_table,\\\n",
    "                                                  climate_risk_columns,\\\n",
    "                                                  futures_columns,\\\n",
    "                                                  include_groups=False\n",
    "                                                 )\n",
    "            corr_tables = corr_tables\\\n",
    "                          .reset_index(level=len(corr_tables.index.levels)-1,drop=True)\\\n",
    "                          .reset_index()\n",
    "            ## compute and combine the correlation table for each group\n",
    "        except KeyError:\n",
    "            print('illegal by values')\n",
    "            return df_default\n",
    "\n",
    "    return corr_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc022dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfcs(df):\n",
    "    \"\"\"\n",
    "    Calculate the Climate-Futures Correlation Score (CFCS) for leaderboard ranking.\n",
    "    \n",
    "    CFCS = (0.5 × Avg_Sig_Corr_Score) + (0.3 × Max_Corr_Score) + (0.2 × Sig_Count_Score)\n",
    "\n",
    "    Input dataframe must have correlation column for computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove null correlations\n",
    "    valid_corrs = df[\"correlation\"].dropna()\n",
    "    \n",
    "    if len(valid_corrs) == 0:\n",
    "        return {'cfcs_score': 0.0, 'error': 'No valid correlations'}\n",
    "    \n",
    "    # Calculate base metrics\n",
    "    abs_corrs = valid_corrs.abs()\n",
    "    max_abs_corr = abs_corrs.max()\n",
    "    significant_corrs = abs_corrs[abs_corrs >= 0.5]\n",
    "    significant_count = len(significant_corrs)\n",
    "    total_count = len(valid_corrs)\n",
    "    \n",
    "    # Calculate component scores - ONLY average significant correlations\n",
    "    if significant_count > 0:\n",
    "        avg_sig_corr = significant_corrs.mean()\n",
    "        avg_sig_score = min(100, avg_sig_corr * 100)  # Cap at 100 when avg sig reaches 1.0\n",
    "    else:\n",
    "        avg_sig_corr = 0.0\n",
    "        avg_sig_score = 0.0\n",
    "    \n",
    "    max_corr_score = min(100, max_abs_corr * 100)  # Cap at 100 when max reaches 1.0\n",
    "    sig_count_score = (significant_count / total_count) * 100  # Percentage\n",
    "    \n",
    "    # Composite score: Focus more on quality of significant correlations\n",
    "    cfcs = (0.5 * avg_sig_score) + (0.3 * max_corr_score) + (0.2 * sig_count_score)\n",
    "    scoreboard = {'cfcs_score': cfcs, 'avg_sig_score': avg_sig_score,\\\n",
    "                  'max_corr_score': max_corr_score,\\\n",
    "                  'sig_count_score': sig_count_score\n",
    "                 }\n",
    "    print(f'{round(sig_count_score,2)}% of all correlations are significant')\n",
    "    print(f'Average significant correlation is {round(avg_sig_corr,3)}')\n",
    "    print(f'highest absolute correlation found is {round(max_abs_corr,3)}')\n",
    "    print(f'final CFCS score is {round(cfcs,2)}')\n",
    "    return scoreboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigcorr_report(df, features='climate_variable', sig_level=0.5):\n",
    "    '''\n",
    "        Generate a CFCS sub scores report for each feature\n",
    "        with significant correlations.\n",
    "        \n",
    "        Input dataframe must contain a correlation column and feature columns.\n",
    "    \n",
    "        Returns a DataFrame with columns:\n",
    "        ['climate_variable', 'avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']\n",
    "    '''\n",
    "    \n",
    "    # Absolute correlation\n",
    "    df['correlation_abs'] = df.correlation.abs()\n",
    "    \n",
    "    try:\n",
    "        # Mask correlations below threshold\n",
    "        df.loc[df['correlation_abs'] < sig_level, ['correlation_abs']] = np.nan\n",
    "        \n",
    "        # Group by feature\n",
    "        reportdf = df.groupby(features).agg({\n",
    "            'correlation_abs': ['mean','max','count'],\n",
    "            'correlation': 'count'\n",
    "        })\n",
    "        \n",
    "    except TypeError:\n",
    "        print('sig_level must be a number between 0 and 1')\n",
    "        return None\n",
    "    except KeyError:\n",
    "        print('illegal features values')\n",
    "        return None\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    reportdf.columns = ['avg_sig_corr','max_sig_corr','sig_corr_count','total_corr_count']\n",
    "    \n",
    "    # Compute ratio\n",
    "    reportdf['sig_corr_ratio(%)'] = 100 * reportdf['sig_corr_count'] / reportdf['total_corr_count']\n",
    "    \n",
    "    # Keep only rows with avg_sig_corr not null\n",
    "    reportdf = reportdf[reportdf['avg_sig_corr'].notnull()]\\\n",
    "                       .loc[:, ['avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']]\\\n",
    "                       .round(3)\n",
    "    \n",
    "    # Add the feature name as a column\n",
    "    reportdf[features] = reportdf.index\n",
    "    reportdf = reportdf.reset_index(drop=True)  # optional: reset index\n",
    "    \n",
    "    # Reorder columns so feature name is first\n",
    "    reportdf = reportdf[[features, 'avg_sig_corr','max_sig_corr','sig_corr_count','sig_corr_ratio(%)']]\n",
    "    \n",
    "    return reportdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_risk_cols = [c for c in pldf.columns if c.startswith(\"climate_risk\")]\n",
    "futures_col = [c for c in pldf.columns if c.startswith(\"futures_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed525",
   "metadata": {},
   "source": [
    "### Reducing the computation time, therefore we did backtesting 200 by 200, instead of a 13000 overall features backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aff8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 200  # number of climate_risk columns per chunk\n",
    "column_chunks = [climate_risk_cols[i:i + chunk_size] for i in range(0, len(climate_risk_cols), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f6236",
   "metadata": {},
   "source": [
    "#### Require to save the files into a folder, please change your file_path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a80b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols = ['ID', 'crop_name', 'country_name', 'date_on_month']\n",
    "\n",
    "for i, feature_chunk in enumerate(column_chunks):\n",
    "    cols_for_corr = futures_col + feature_chunk + extra_cols\n",
    "    pdf_chunk = pldf.select(cols_for_corr).to_pandas()\n",
    "\n",
    "    corrtable = compute_partial_correlations(pdf_chunk)\n",
    "    table = sigcorr_report(corrtable).sort_values(by='avg_sig_corr', ascending=False)\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    output_dir = \"/kaggle/working/corr_files\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # exist_ok=True avoids error if folder already exists\n",
    "    \n",
    "    # Then you can save your CSV\n",
    "    table.to_csv(f\"{output_dir}/corr_report_cols_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d71e5e",
   "metadata": {},
   "source": [
    "#### After the 1 hour running time of corr_report_cols_{i}, we merge them to have all features cfcs result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "# Folder containing your CSVs\n",
    "csv_folder = \"/kaggle/working/corr_files\"\n",
    "\n",
    "# Get all CSV file paths\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "\n",
    "# Read all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "# Drop fully empty columns\n",
    "dfs_clean = [df.dropna(axis=1, how='all') for df in dfs]\n",
    "\n",
    "# Concatenate safely row-wise\n",
    "result_table = pd.concat(dfs_clean, axis=0, ignore_index=True)\n",
    "\n",
    "# Check\n",
    "print(result_table.shape)\n",
    "print(result_table.head())\n",
    "\n",
    "result_table.to_csv('final_result_corr_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c2988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
